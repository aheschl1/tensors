use std::marker::PhantomData;

use crate::backend::Backend;
use crate::backend::cpu::Cpu;
use crate::core::value::TensorValue;
use crate::core::{shape_to_stride, Shape, MetaTensor};
use crate::core::tensor::TensorError;

/// A generic tensor with backend-specific storage.
/// 
/// This is the base type for all tensors, parameterized by element type `T` and backend `B`.
/// Most users will use type aliases like `Tensor<T>` (CPU) or `CudaTensor<T>` (GPU).
#[derive(Debug, PartialEq, Eq)]
pub struct TensorBase<T: TensorValue, B: Backend<T>> {
    pub(crate) backend: B,
    pub(crate) buf: B::Buf,
    pub(crate) meta: MetaTensor,
    _t: PhantomData<T>,
}

impl<B: Backend<T>, T: TensorValue> Clone for TensorBase<T, B> {
    fn clone(&self) -> Self {
        let new_backend = B::new();
        let new_buffer = new_backend.copy(&self.buf).unwrap();
        Self {
            backend: new_backend,
            buf: new_buffer,
            meta: self.meta.clone(),
            _t: PhantomData,
        }

    }
}

/// An owned CPU tensor stored in row-major order.
/// 
/// # Examples
/// ```ignore
/// let tensor = Tensor::<f32>::zeros((3, 4));
/// let tensor = Tensor::<i32>::from_buf(vec![1, 2, 3, 4], (2, 2)).unwrap();
/// ```
pub type Tensor<T> = TensorBase<T, Cpu>;

#[cfg(feature = "cuda")]
/// An owned GPU tensor stored on CUDA device.
pub type CudaTensor<T> = TensorBase<T, crate::backend::cuda::Cuda>;

#[cfg(feature = "cuda")]
impl<T: TensorValue> CudaTensor<T> {
    /// Transfers this tensor from the CUDA device to CPU memory.
    pub fn cpu(&self) -> Result<Tensor<T>, TensorError> {
        let cpu_backend = Cpu;
        let cpu_buffer = self.backend.dump(&self.buf)?;
        let cpu = Tensor::from_parts(cpu_backend, cpu_buffer, self.meta.clone());
        Ok(cpu)
    }
}

#[cfg(feature = "cuda")]
impl<T: TensorValue> Tensor<T> {
    /// Transfers this tensor from CPU to the CUDA device.
    pub fn cuda(&self) -> Result<CudaTensor<T>, TensorError> {
        let cuda_backend = crate::backend::cuda::Cuda::construct(0)?;
        let cuda_buffer = cuda_backend.alloc_from_slice(self.backend.dump(&self.buf)?)?;
        let cuda = CudaTensor::from_parts(cuda_backend, cuda_buffer, self.meta.clone());
        Ok(cuda)
    }
}

/// A non-owning immutable view over tensor data.
/// 
/// Views share the underlying buffer with the source tensor and have their own
/// metadata (shape, stride, offset) to represent different interpretations of the data.
pub struct TensorView<'a, T, B>
where
    T: TensorValue,
    B: Backend<T> + 'a,
{
    pub(crate) buf: &'a B::Buf,
    pub(crate) backend: &'a B,
    pub(crate) meta: MetaTensor,
}

/// A non-owning mutable view over tensor data.
/// 
/// Like `TensorView` but allows mutation of the underlying data.
pub struct TensorViewMut<'a, T, B>
where
    T: TensorValue,
    B: Backend<T> + 'a,
{
    pub(crate) buf: &'a mut B::Buf,
    pub(crate) backend: &'a B,
    pub(crate) meta: MetaTensor,
}

impl<'a, T, B> TensorView<'a, T, B>
where
    T: TensorValue,
    B: Backend<T> + 'a,
{
    /// Builds a tensor view from raw storage and metadata. No copying occurs;
    /// caller guarantees that `meta` correctly describes the layout within `raw`.
    pub(crate) fn from_parts(
        buf: &'a B::Buf,
        backend: &'a B,
        meta: MetaTensor
    ) -> Self {
        Self {
            buf,
            backend,
            meta,
        }
    }
}

impl<'a, T, B> TensorViewMut<'a, T, B>
where
    T: TensorValue,
    B: Backend<T> + 'a,
{
    /// Builds a tensor view from raw storage and metadata. No copying occurs;
    /// caller guarantees that `meta` correctly describes the layout within `raw`.
    pub(crate) fn from_parts(
        raw: &'a mut B::Buf,
        backend: &'a B,
        meta: MetaTensor
    ) -> Self {
        Self {
            buf: raw,
            backend,
            meta
        }
    }
}

pub type CpuTensorView<'a, T> = TensorView<'a, T, Cpu>;
pub type CpuTensorViewMut<'a, T> = TensorViewMut<'a, T, Cpu>;
#[cfg(feature = "cuda")]
pub type CudaTensorView<'a, T> = TensorView<'a, T, crate::backend::cuda::Cuda>;
#[cfg(feature = "cuda")]
pub type CudaTensorViewMut<'a, T> = TensorViewMut<'a, T, crate::backend::cuda::Cuda>;

impl<B, T: TensorValue> TensorBase<T, B> 
where 
    B: Backend<T>,
{
    /// Internal constructor from raw parts. Used for creating tensors from
    /// existing backend buffers without copying.
    pub(crate) fn from_parts(backend: B, raw: B::Buf, meta: MetaTensor) -> Self {
        Self {
            backend,
            buf: raw,
            meta,
            _t: PhantomData,
        }
    }

    /// Constructs a tensor from a buffer and shape.
    /// 
    /// The buffer must be contiguous and in row-major order.
    /// 
    /// # Errors
    /// - `InvalidShape` if the buffer size doesn't match the shape.
    /// - `InvalidShape` if the shape has more than 128 dimensions.
    /// 
    /// # Examples
    /// ```ignore
    /// let tensor = Tensor::<f32>::from_buf(vec![1.0, 2.0, 3.0, 4.0], (2, 2)).unwrap();
    /// ```
    pub fn from_buf(raw: impl Into<Box<[T]>>, shape: impl Into<Shape>) -> Result<Self, TensorError> {
        let shape: Shape = shape.into();
        if shape.len() > 128 {
            // artificial cap due to broadcast cuda kernel...
            return Err(TensorError::InvalidShape(format!(
                "Tensors with more than 128 dimensions are not supported, got {} dimensions",
                shape.len()
            )));
        }
        let backend = B::new();
        let buffer = backend.alloc_from_slice(raw.into())?;
        if shape.iter().product::<usize>() != backend.len(&buffer) {
            return Err(TensorError::InvalidShape(format!(
                "Element count mismatch: shape implies {} elements, but buffer has {} elements",
                shape.iter().product::<usize>(),
                backend.len(&buffer)
            )));
        }
        let stride = shape_to_stride(&shape);
        Ok(Self {
            backend,
            buf: buffer,
            meta: MetaTensor::new(shape, stride, 0),
            _t: PhantomData,
        })
    }

    /// Creates a rank-0 (scalar) tensor.
    /// 
    /// # Examples
    /// ```ignore
    /// let scalar = Tensor::<f32>::scalar(42.0);
    /// ```
    pub fn scalar(value: T) -> Self {
        Self::from_buf(vec![value], vec![]).unwrap()
    }

    /// Creates a 1-D column tensor from values.
    /// 
    /// # Examples
    /// ```ignore
    /// let col = Tensor::<i32>::column(vec![1, 2, 3]);
    /// ```
    pub fn column(column: impl Into<Box<[T]>>) -> Self {
        let column = column.into();
        let shape = vec![column.len()];
        Self::from_buf(column, shape).unwrap()
    }

    /// Creates a 1xN row tensor from values.
    /// 
    /// # Examples
    /// ```ignore
    /// let row = Tensor::<f32>::row(vec![1.0, 2.0, 3.0]);
    /// ```
    pub fn row(row: impl Into<Box<[T]>>) -> Self {
        let row = row.into();
        let shape = vec![1, row.len()];
        Self::from_buf(row, shape).unwrap()
    }

    /// Creates a tensor filled with zeros.
    /// 
    /// # Panics
    /// Panics if memory allocation fails.
    /// 
    /// # Examples
    /// ```ignore
    /// let zeros = Tensor::<f32>::zeros((3, 4));
    /// ```
    pub fn zeros(shape: impl Into<Shape>) -> Self {
        let shape: Shape = shape.into();
        let element_count = shape.iter().product::<usize>();
        let zero_buf = vec![T::zero(); element_count];
        Self::from_buf(zero_buf, shape).expect("Failed to allocate memory")
    }

    /// Creates a tensor filled with ones.
    /// 
    /// # Panics
    /// Panics if memory allocation fails.
    /// 
    /// # Examples
    /// ```ignore
    /// let ones = Tensor::<f32>::ones((2, 2));
    /// ```
    pub fn ones(shape: impl Into<Shape>) -> Self {
        let shape: Shape = shape.into();
        let element_count = shape.iter().product::<usize>();
        let one_buf = vec![T::one(); element_count];
        Self::from_buf(one_buf, shape).expect("Failed to allocate memory")
    }

    /// Creates a tensor filled with the maximum value for type `T`.
    /// 
    /// # Panics
    /// Panics if memory allocation fails.
    pub fn max(shape: impl Into<Shape>) -> Self {
        let shape: Shape = shape.into();
        let element_count = shape.iter().product::<usize>();
        let max_buf = vec![T::max(); element_count];
        Self::from_buf(max_buf, shape).expect("Failed to allocate memory")
    }

    /// Creates a tensor filled with the minimum value for type `T`.
    /// 
    /// # Panics
    /// Panics if memory allocation fails.
    pub fn min(shape: impl Into<Shape>) -> Self {
        let shape: Shape = shape.into();
        let element_count = shape.iter().product::<usize>();
        let min_buf = vec![T::min(); element_count];
        Self::from_buf(min_buf, shape).expect("Failed to allocate memory")
    }

}

/// Indicates where a tensor's data resides.
#[derive(Debug, Clone, Copy, PartialEq, Eq)]
pub enum DeviceType {
    /// CPU memory
    Cpu,
    #[cfg(feature = "cuda")]
    /// CUDA device memory (GPU), with device index
    Cuda(usize),
}