use crate::{backend::Backend, core::{idx::Idx, primitives::TensorBase, shape_to_stride, tensor::{AsTensor, AsView, TensorError}, value::{TensorValue, WeightValue}, MetaTensor, MetaTensorView, Shape}};


pub enum ReductionOpTypes {
    Sum,
    Prod,
    Max,
    Min,
    Mean,
    Variance {
        unbiased: bool
    },
    Stdev {
        unbiased: bool
    },
    LogSumExp,
    Norm(NormType)
}

impl ReductionOpTypes {
    #[inline(always)]
    pub const fn get_code(&self) -> u8 {
        match self {
            Self::Sum => 1,
            Self::Prod => 2,
            Self::Max => 3,
            Self::Min => 4,
            Self::Mean => 5,
            Self::Variance {..} | Self::Stdev { .. } => 6,
            Self::LogSumExp => 7,
            Self::Norm(_) => 8
            
        }
    }

    #[inline(always)]
    pub fn fold<T: TensorValue>(&self, a: T, b: T) -> T {
        match self {
            Self::Sum => a + b,
            Self::Prod => a * b,
            Self::Max => if a > b { a } else { b },
            Self::Min => if a < b { a } else { b },
            _ => panic!("Fold not implemented for this reduction type.")
        }
    }

    #[inline(always)]
    pub const fn initial_value<T: TensorValue>(&self) -> T {
        match self {
            Self::Sum => T::ZERO,
            Self::Prod => T::ONE,
            Self::Max => T::MIN,
            Self::Min => T::MAX,
            _ => panic!("Initial value not implemented for this reduction type.")
        }
    }
}

pub enum NormType {
    L1,
    L2
}

pub trait TotalReductionOp<T: TensorValue, B: Backend>: Sized + ReductionOp<T, B> {
    fn total_sum(&self) -> Result<TensorBase<T, B>, TensorError> {self.sum(Idx::Item)}
    fn total_prod(&self) -> Result<TensorBase<T, B>, TensorError> {self.prod(Idx::Item)}
    fn total_mean(&self) -> Result<TensorBase<T, B>, TensorError> {self.mean(Idx::Item)}
    fn total_max(&self) -> Result<TensorBase<T, B>, TensorError>{self.max(Idx::Item)}
    fn total_min(&self) -> Result<TensorBase<T, B>, TensorError>{self.min(Idx::Item)}
    fn total_var(&self) -> Result<TensorBase<T, B>, TensorError>{self.var(Idx::Item)}
    fn total_pop_var(&self) -> Result<TensorBase<T, B>, TensorError>{self.pop_var(Idx::Item)}
    fn total_std(&self, unbiased: bool) -> Result<TensorBase<T, B>, TensorError>{self.std(Idx::Item, unbiased)}
    fn total_norm(&self, norm: NormType) -> Result<TensorBase<T, B>, TensorError>{self.norm(Idx::Item, norm)}
    fn total_logsumexp(&self) -> Result<TensorBase<T, B>, TensorError>{self.logsumexp(Idx::Item)}
    fn total_l1_norm(&self) -> Result<TensorBase<T, B>, TensorError> {
        self.norm(Idx::Item, NormType::L1)
    }
    fn total_l2_norm(&self) -> Result<TensorBase<T, B>, TensorError> {
        self.norm(Idx::Item, NormType::L2)
    }
}

pub trait ReductionOp<T: TensorValue, B: Backend> : Sized {
    
    fn sum(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn prod(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn mean(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn max(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn min(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn var(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn pop_var(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;
    fn std(&self, axes: impl Into<Idx>, unbiased: bool) -> Result<TensorBase<T, B>, TensorError>;
    fn norm(&self, axes: impl Into<Idx>, norm: NormType) -> Result<TensorBase<T, B>, TensorError>;
    fn logsumexp(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError>;

    fn l1_norm(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        self.norm(axes, NormType::L1)
    }

    fn l2_norm(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        self.norm(axes, NormType::L2)
    }
}

impl<T: TensorValue, B: Backend, V> TotalReductionOp<T, B> for V where V: ReductionOp<T, B>{}

#[inline(always)]
pub fn do_reduce<T, B>(
    op: ReductionOpTypes,
    axes: &Idx,
    tensor: &impl AsView<T, B>,
) -> Result<TensorBase<T, B>, TensorError>
where
    T: WeightValue,
    B: Backend,
{
    let tensor = tensor.view();
    match axes {
        Idx::Item => {
            let mut output = TensorBase::from_buf(vec![T::ZERO], vec![])?;
            tensor.backend.apply_reduce_contiguous_flat(
                tensor.buf,
                &mut output.buf,
                tensor.meta.offset,
                tensor.meta.size(),
                op,
            )?;
            Ok(output)
        }
        Idx::At(axis) => {
            let mut output =
                materialize_output::<T, B>(&tensor.meta, tensor.backend.clone(), axes)?;
            tensor.backend.apply_reduce(
                (tensor.buf, &tensor.meta),
                (&mut output.buf, &output.meta),
                *axis,
                op,
            )?;
            Ok(output)
        }
        _ => Err(TensorError::WrongDims(
            "Reduction over multiple axes is not implemented yet.".to_string(),
        )),
    }
}


impl<T: WeightValue, B: Backend, V> ReductionOp<T, B> for V
where
    V: AsView<T, B>,
{
    fn sum(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(ReductionOpTypes::Sum, &axes, &a)
        } else {
            do_reduce(ReductionOpTypes::Sum, &axes, &t)
        }
    }

    fn prod(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(ReductionOpTypes::Prod, &axes, &a)
        } else {
            do_reduce(ReductionOpTypes::Prod, &axes, &t)
        }
    }

    fn max(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(ReductionOpTypes::Max, &axes, &a)
        } else {
            do_reduce(ReductionOpTypes::Max, &axes, &t)
        }
    }

    fn min(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(ReductionOpTypes::Min, &axes, &a)
        } else {
            do_reduce(ReductionOpTypes::Min, &axes, &t)
        }
    }

    fn mean(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(ReductionOpTypes::Mean, &axes, &a)
        } else {
            do_reduce(ReductionOpTypes::Mean, &axes, &t)
        }
    }
    fn var(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let code = ReductionOpTypes::Variance { unbiased: true };
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(code, &axes, &a)
        }else {
            do_reduce(code, &axes, &t)
        }
    }
    fn pop_var(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let code = ReductionOpTypes::Variance { unbiased: false };
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(code, &axes, &a)
        }else {
            do_reduce(code, &axes, &t)
        }
    }
    fn std(&self, axes: impl Into<Idx>, unbiased: bool) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let code = ReductionOpTypes::Stdev { unbiased };
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(code, &axes, &a)
        }else {
            do_reduce(code, &axes, &t)
        }
    }
    fn logsumexp(&self, axes: impl Into<Idx>) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(ReductionOpTypes::LogSumExp, &axes, &a)
        }else {
            do_reduce(ReductionOpTypes::LogSumExp, &axes, &t)
        }
    }
    fn norm(&self, axes: impl Into<Idx>, norm: NormType) -> Result<TensorBase<T, B>, TensorError> {
        let axes = axes.into();
        let code = ReductionOpTypes::Norm(norm);
        let t = self.view();
        if !t.is_contiguous() {
            let a = t.contiguous();
            do_reduce(code, &axes, &a)
        }else {
            do_reduce(code, &axes, &t)
        }
    }
}

#[inline]
fn materialize_output<T: TensorValue, B: Backend>(input: &MetaTensor, backend: B, axes: &Idx) -> Result<TensorBase<T, B>, TensorError>{
    let output_meta = reduction_output_meta(input.clone(), axes);
    let buf = backend.alloc(output_meta.size());

   
    Ok(TensorBase::from_parts(backend, buf?, output_meta))
}

#[inline]
fn reduction_output_meta(input: MetaTensor, axes: &Idx) -> MetaTensor {
    let mut output_shape = Vec::new();


    let idx_num = match axes {
        Idx::Item => 0,
        Idx::At(x) => *x,
        _ => panic!("Weird multi dimension reductions are not avaiable.")
    };




    for d in 0..input.rank() {
        if d == idx_num {
            // PyTorch and Numpy have a keep dim argument that removes this.
            output_shape.push(1);
        } else {
            output_shape.push(input.shape()[d]);

        }
    }

    // for (i, &dim) in input.shape.iter().enumerate() {
    //     println!("i={i}, dim={dim}");
    //     if i < idx_num {
    //         output_shape.push(dim);
    //     }
    // }
    println!("materialziing output: {:?}", output_shape);
    let output_shape: Shape = Shape::from(output_shape);
    let strides = shape_to_stride(&output_shape);

    MetaTensor {
        shape: output_shape,
        strides,
        offset: 0
    }
}